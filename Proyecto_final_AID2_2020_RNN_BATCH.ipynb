{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proyecto_final_AID2_2020_RNN_BATCH.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPLaqI74DNS8dGB5o1wQGMB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samuy93/aid2_proyecto_nlp/blob/main/Proyecto_final_AID2_2020_RNN_BATCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue0GNfCj7tb3"
      },
      "source": [
        "# Proyecto Final AID2 2020\n",
        "El proyecto se basa en el Data Challenge 2019, Mercado Libre\n",
        "Nombre: Samuel Astol"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IMAh5-w4KBT",
        "outputId": "4ec1a3ff-9bf4-4a52-e3e8-3f1c80d67119"
      },
      "source": [
        "# Natural Language Tool Kit\n",
        "!pip install nltk\n",
        "!pip install requests\n",
        "!pip install pandas\n",
        "!pip install DataLoader\n",
        "!pip install torch"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /Users/samuel/anaconda3/lib/python3.8/site-packages (3.5)\r\n",
            "Requirement already satisfied: tqdm in /Users/samuel/anaconda3/lib/python3.8/site-packages (from nltk) (4.50.2)\r\n",
            "Requirement already satisfied: click in /Users/samuel/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\r\n",
            "Requirement already satisfied: regex in /Users/samuel/anaconda3/lib/python3.8/site-packages (from nltk) (2020.10.15)\r\n",
            "Requirement already satisfied: joblib in /Users/samuel/anaconda3/lib/python3.8/site-packages (from nltk) (0.17.0)\n",
            "Requirement already satisfied: requests in /Users/samuel/anaconda3/lib/python3.8/site-packages (2.24.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /Users/samuel/anaconda3/lib/python3.8/site-packages (from requests) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/samuel/anaconda3/lib/python3.8/site-packages (from requests) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/samuel/anaconda3/lib/python3.8/site-packages (from requests) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/samuel/anaconda3/lib/python3.8/site-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: pandas in /Users/samuel/anaconda3/lib/python3.8/site-packages (1.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/samuel/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /Users/samuel/anaconda3/lib/python3.8/site-packages (from pandas) (1.19.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /Users/samuel/anaconda3/lib/python3.8/site-packages (from pandas) (2020.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/samuel/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: DataLoader in /Users/samuel/anaconda3/lib/python3.8/site-packages (2.0)\n",
            "Requirement already satisfied: torch in /Users/samuel/anaconda3/lib/python3.8/site-packages (1.7.0)\n",
            "Requirement already satisfied: future in /Users/samuel/anaconda3/lib/python3.8/site-packages (from torch) (0.18.2)\n",
            "Requirement already satisfied: dataclasses in /Users/samuel/anaconda3/lib/python3.8/site-packages (from torch) (0.6)\n",
            "Requirement already satisfied: typing-extensions in /Users/samuel/anaconda3/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /Users/samuel/anaconda3/lib/python3.8/site-packages (from torch) (1.19.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14x75IoksM5w",
        "outputId": "0ec16c49-8dc5-4237-949e-f31cc5b1893a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/samuel/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/samuel/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXDSGA4c3iWH"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "import string\n",
        "import requests\n",
        "import gzip\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5rYXPKxX0OT"
      },
      "source": [
        "# Descarga y Carga del Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXNb2Ma2XtJ_"
      },
      "source": [
        "# Downlad files\n",
        "def download_file(url, destination):\n",
        "    session = requests.Session()\n",
        "    response = session.get(url, stream = True)\n",
        "    save_response_content(response, destination)\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "download_file(\"https://meli-data-challenge.s3.amazonaws.com/train.csv.gz\" , 'dataset.gz')\n",
        "download_file(\"https://meli-data-challenge.s3.amazonaws.com/test.csv\" , 'datasetTest.csv')"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a_xqeT5XwtH"
      },
      "source": [
        "# read gz file\n",
        "dataset_train = None\n",
        "with gzip.open('dataset.gz') as f:\n",
        "    dataset_train = pd.read_csv(f)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsfMzfxjXhQb"
      },
      "source": [
        "# Definir entorno de ejecucion "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WCNv0xw18wz"
      },
      "source": [
        "is_developer_mode = True\n",
        "max_groups_evaluation = 3"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeejjBP233iL"
      },
      "source": [
        "def is_group_iteration_topped(index):\n",
        "  return is_developer_mode and  index >= max_groups_evaluation"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g07U3bJQX76g"
      },
      "source": [
        "# Definir Pre-Procesamiento del Dataset\n",
        "\n",
        "En esta etapa se procede a normalizar el dataset\n",
        "\n",
        "* removemos elementos vacios\n",
        "* removemos signos de puntuacion\n",
        "* removemos stopwords (espano y portugues)\n",
        "\n",
        "Notas: \n",
        "* se considera todo el dataset como un solo lenguaje, para el caso de estudio pero se entiende que a la hora de predecir los resultados en espa√±ol o portugues pueda tener una considerable preferencia a a predecir correcto en uno de los lenguajes.\n",
        "* para el proposito del proyecto se tomo por clase un total de samples igual al minimo encontrado en todas las classes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALPCnGj8Q7E5"
      },
      "source": [
        "# tokenize and remove stopwords from str list\n",
        "class Custom_Str_Sanitizer:\n",
        "  def remove_punctuation(self, text):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(table)\n",
        "\n",
        "  def remove_stop_words(self, titles, language):\n",
        "    stop_words = set(stopwords.words(language.lower()))\n",
        "    return [w for w in titles if w not in stop_words]\n",
        "\n",
        "  def sanitize_dataset(self, titles, language):\n",
        "    to_return = []\n",
        "    for title in titles:\n",
        "      title = self.remove_stop_words([word.lower() for word in word_tokenize(self.remove_punctuation(title))], language)\n",
        "      title = [word for word in title if word.isalpha()]\n",
        "      to_return.append(title)\n",
        "    return toReturn\n",
        "\n",
        "  def start(self, titles, language):\n",
        "    return self.sanitize_dataset(titles, language)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj5Ubi9-cIA3"
      },
      "source": [
        "# tokenize and remove stopwords\n",
        "class Custom_Sanitizer:\n",
        "  def remove_punctuation(self, text):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(table)\n",
        "\n",
        "  def remove_top_words(self, lst, language):\n",
        "    stop_words = set(stopwords.words(language.lower()))\n",
        "    return [w for w in lst if w not in stop_words]\n",
        "\n",
        "  def sanitize_dataset(self, df):\n",
        "    corpus = []\n",
        "    newDf = df.copy(deep=True)\n",
        "    for index, row in newDf.iterrows():\n",
        "      row.title = self.remove_top_words([word.lower() for word in word_tokenize(self.remove_punctuation(row.title))], row.language)\n",
        "      #row.title = [word for word in row.title if word.isalpha()]\n",
        "      corpus.append(row.title)\n",
        "    return newDf, corpus\n",
        "\n",
        "  def start(self, df):\n",
        "    return self.sanitize_dataset(df)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCxu3ieYdq59"
      },
      "source": [
        "# Definicinon del Vocabulario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnxgZZ6hU_q8"
      },
      "source": [
        "class Vocabulary:\n",
        "  def __init__(self, corpus):\n",
        "    self.corpus = corpus\n",
        "    self.vect_corpus = []\n",
        "    self.vocab = []\n",
        "\n",
        "  def built(self):\n",
        "    for doc in self.corpus:\n",
        "      self.add_doc(doc)\n",
        "\n",
        "  def add_doc(self, doc):\n",
        "    vocab_set = set(self.vocab)\n",
        "    doc_set = set(doc)\n",
        "    diff = doc_set.difference(vocab_set)\n",
        "    new_tokens = [token for token in doc if token in diff]\n",
        "    self.vocab = self.vocab + new_tokens\n",
        "    self.vect_corpus.append([self.get_index_of_token(token) for token in doc])\n",
        "\n",
        "  def get_token_by_index(self, index):\n",
        "    return self.vocab[index]\n",
        "\n",
        "  def get_index_of_token(self, token):\n",
        "    return self.vocab.index(token)\n",
        "      "
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy_uZO_-DpxY"
      },
      "source": [
        "class Data_Set:\n",
        "  def remove_rows_with_empty_col(self, colName):\n",
        "    self.df[colName].replace('', np.nan, inplace=True)\n",
        "    self.df.dropna(subset=[colName], inplace=True)\n",
        "\n",
        "  def calculate_min_and_avg_group(self, dt):\n",
        "    category_row_size = []\n",
        "    minVal = -1;\n",
        "    print(\"Num of groups: %s\" % len(dt.groups))\n",
        "    group_index = 0\n",
        "    for category in dt.groups:\n",
        "      if is_group_iteration_topped(group_index):\n",
        "        break\n",
        "      group = dt.get_group(category)\n",
        "      rows, cols = group.shape\n",
        "      category_row_size.append(rows)\n",
        "      if minVal == -1:\n",
        "        minVal = rows\n",
        "      elif minVal > rows:\n",
        "        minVal = rows\n",
        "      group_index = group_index + 1\n",
        "      \n",
        "    min_rows_per_category = min(category_row_size)\n",
        "    avg_rows_per_category = sum(category_row_size)/len(category_row_size)\n",
        "    print(\"Min num of rows in categories: %s\" % min_rows_per_category)\n",
        "    print(\"Avg num of rows in categories: %s\" % avg_rows_per_category)\n",
        "    return min_rows_per_category, avg_rows_per_category\n",
        "\n",
        "  def set_even_rows_per_group(self, df, min):\n",
        "    toReturn = pd.DataFrame(columns=[\"title\",\"label_quality\",\"language\",\"category\"])\n",
        "    group_index = 0\n",
        "    for category in df.groups:\n",
        "      if is_group_iteration_topped(group_index):\n",
        "        break\n",
        "      group = df.get_group(category).take(range(min))\n",
        "      toReturn = toReturn.append(group, ignore_index=True)\n",
        "      group_index = group_index + 1\n",
        "    return toReturn\n",
        "\n",
        "  def transform(self, df):\n",
        "    self.df = df\n",
        "    self.remove_rows_with_empty_col('title')\n",
        "    self.remove_rows_with_empty_col('category')\n",
        "    by_categories = self.df.groupby(by='category', as_index=True)\n",
        "    min_rows_per_category, avgRowsavg_rows_per_categoryPerCategory = self.calculate_min_and_avg_group(by_categories)\n",
        "    newdf = self.set_even_rows_per_group(by_categories, min_rows_per_category)\n",
        "    sanitazer = Custom_Sanitizer()\n",
        "    df_sanitized, corpus = sanitazer.start(newdf)\n",
        "    all_categories = list(by_categories.groups)[:max_groups_evaluation] if is_developer_mode else list(by_categories.groups)\n",
        "    mapping_categories = df_sanitized['category']\n",
        "    return df_sanitized, corpus, all_categories, mapping_categories\n",
        "      "
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q1uoPUIccvf"
      },
      "source": [
        "# Modelo RNN + Word Embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pFjwdKtOGHK"
      },
      "source": [
        "# many to one\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(RNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "    self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, input_tensor, hidden_tensor):\n",
        "    combined = torch.cat((input_tensor, hidden_tensor), 1)\n",
        "    hidden = self.i2h(combined)\n",
        "    output = self.i2o(combined)\n",
        "    output = self.softmax(output)\n",
        "    return output, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.5\n",
        "    self.i2o.weight.data.uniform_(-initrange, initrange)\n",
        "    self.i2o.bias.data.zero_()\n",
        "    self.i2h.weight.data.uniform_(-initrange, initrange)\n",
        "    self.i2h.bias.data.zero_()"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y54kFwIAc48C"
      },
      "source": [
        "# Parametros de entrenamiento\n",
        "\n",
        "*   Ejecucion de pre-procesamiento\n",
        "*   Armado del vocabulario\n",
        "*   Sete de Parametros de entrenamieto\n",
        "*   Padding de los vectores \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSMK8KmPGw1q"
      },
      "source": [
        "# sanitaze corpus (remove stop words, convert to lowercase, same documents per label)\n",
        "ds = Data_Set()\n",
        "df_sanitized, corpus, all_categories, mapping_categories = ds.transform(dataset_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iQLkT4hLwVB"
      },
      "source": [
        "# built corpus vector\n",
        "vocab = Vocabulary(corpus)\n",
        "vocab.built()"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ1DYACbFith"
      },
      "source": [
        "# add padding to all docs, make all docs same len\n",
        "max_len_vect_doc = max([len(vect_doc) for vect_doc in vocab.vect_corpus])\n",
        "vect_corpus_padded = np.array([np.pad(vect_doc, (0, max_len_vect_doc - len(vect_doc)), 'constant', constant_values=0) for vect_doc in vocab.vect_corpus])\n"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPkSOMgnk-2e"
      },
      "source": [
        "trains_x, tests_x, trains_y, tests_y = model_selection.train_test_split(vect_corpus_padded, mapping_categories, test_size=0.3)\n"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opDXItjLQrCe"
      },
      "source": [
        "input_size = len(vect_corpus_padded[0])\n",
        "embedding_dim = 50\n",
        "hidden_size = 50\n",
        "learning_rate = 0.05\n",
        "n_iters = 100\n",
        "plot_steps, print_steps = 10, 10\n",
        "output_size = len(all_categories)\n"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vNp35Nwxebo"
      },
      "source": [
        "embedding = nn.Embedding(len(vocab.vocab), embedding_dim, sparse=True)\n",
        "rnn = RNN(input_size * embedding_dim, hidden_size, output_size)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikm2YSnETng7"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr = learning_rate)\n",
        "def train(embedded_document, class_tensor):\n",
        "  hidden = rnn.init_hidden()\n",
        "  output, hidden = rnn(embedded_document.view(1,-1), hidden)\n",
        "  loss = criterion(output, class_tensor)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return output, loss.item()"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gB5YTBSwkQEK",
        "outputId": "e1c98762-d3ee-491c-afce-8af8f167d27c"
      },
      "source": [
        "current_loss = 0\n",
        "all_losses = []\n",
        "rnn.init_weights()\n",
        "embedding.weight.data.uniform_(-0.5, 0.5)\n",
        "trains_y = list(trains_y)\n",
        "tests_y = list(tests_y)\n",
        "for i in range(n_iters):\n",
        "  for index, doc_padded in enumerate(trains_x):\n",
        "    embedded_document = embedding(torch.tensor(doc_padded))\n",
        "    class_tensor = []\n",
        "    class_index = all_categories.index(trains_y[index])\n",
        "    class_tensor.append(class_index)\n",
        "    output, loss = train(torch.flatten(embedded_document), torch.tensor(class_tensor))\n",
        "    current_loss = current_loss + loss\n",
        "\n",
        "  if (i+1) % plot_steps == 0:\n",
        "    print(current_loss / plot_steps)\n",
        "    all_losses.append(current_loss / plot_steps)\n",
        "    current_loss = 0"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "785.3199695408344\n",
            "675.5065070271492\n",
            "652.4309735357762\n",
            "642.3600073993206\n",
            "637.1042136013508\n",
            "633.3594088494777\n",
            "629.7903100430965\n",
            "627.3323191583156\n",
            "625.2710709750652\n",
            "622.9934014439583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dliZcF2CI1D"
      },
      "source": [
        "print(mapping_categories[1000:1050])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tARmf9V3BMjV"
      },
      "source": [
        "\n",
        "pred_index = 1048\n",
        "print(tests_[pred_index])\n",
        "print(mapping_categories[pred_index])\n",
        "print([vocab.get_token_by_index(index) for index in vect_corpus_padded[pred_index]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FJdAGUXA_Su"
      },
      "source": [
        "\n",
        "embedded_document = embedding(torch.tensor(vect_corpus_padded[pred_index]))\n",
        "class_tensor = []\n",
        "class_index = all_categories.index(mapping_categories[pred_index])\n",
        "class_tensor.append(class_index)\n",
        "output, loss = train(torch.flatten(embedded_document), torch.tensor(class_tensor))\n",
        "category_idx = torch.argmax(output).item()\n",
        "print(all_categories[category_idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QVkGlOA7vwa"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaNDmemrjLDe"
      },
      "source": [
        "# Prueba de Predicciones\n",
        "\n",
        "Nota: se uso el mismo data set de entrenamiendo por temas de timepo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Nhqs-GjGDN"
      },
      "source": [
        "def predict():\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    hidden = rnn.init_hidden()\n",
        "    aciertos = 0\n",
        "    no_aciertos = 0\n",
        "\n",
        "    for index, test_x in enumerate(tests_x):\n",
        "      embedded_document = embedding(torch.tensor(test_x))\n",
        "      output, hidden = rnn(torch.flatten(embedded_document).view(1,-1), hidden)\n",
        "      if tests_y[index] == all_categories[torch.argmax(output).item()]:\n",
        "        aciertos += 1\n",
        "      else :\n",
        "        no_aciertos += 1\n",
        "\n",
        "    print('num aciertos')\n",
        "    print(aciertos)\n",
        "    print('num no aciertos')\n",
        "    print(no_aciertos)"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPd6ZBB5tQon",
        "outputId": "fbcf5241-3c57-4b76-94f5-6a2957c2059c"
      },
      "source": [
        "predict()"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num aciertos\n",
            "159\n",
            "num no aciertos\n",
            "298\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}